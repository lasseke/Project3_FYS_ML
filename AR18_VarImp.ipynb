{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate variable importance for the AR18_18 models\n",
    "### Fit RF and NN to same subset of data and calculate permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import relevant packages\n",
    "# Matrix/vector handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-learn and RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# Permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Colinearity\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "from collections import defaultdict\n",
    "\n",
    "# NN\n",
    "from lib import NeuralNet\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize Onehotencoder and Scaler\n",
    "onehotencoder = OneHotEncoder(categories=\"auto\")\n",
    "sc = StandardScaler()\n",
    "\n",
    "### Read confusion matrices, one hot-encoded (for NN), one not (decision trees)\n",
    "X_cat = pd.read_pickle(\"./DataFiles/FeatureMatrixCats.pkl\")\n",
    "X_dum = pd.read_pickle(\"./DataFiles/FeatureMatrixDummy.pkl\")\n",
    "\n",
    "# Read target\n",
    "y_cat = pd.read_pickle(\"./DataFiles/TargetVTs.pkl\")\n",
    "y_dum = LabelBinarizer().fit_transform(y_cat)\n",
    "\n",
    "### Normalize the NN feature matrix\n",
    "X_dum_scaled = np.array(X_dum)\n",
    "X_dum_scaled[:,3:65] = sc.fit_transform(X_dum_scaled[:,3:65]) # Omit x/y/plotid and cat. dummy variables\n",
    "\n",
    "### Construct final feature matrices\n",
    "\n",
    "# Remove variables from data frames that should not be in feature matrix\n",
    "rmvars = [\"x\",\"y\",\"plot_id\",\"geology_norge1\"]\n",
    "X_rf = X_cat.drop(rmvars, axis=1)\n",
    "\n",
    "# Remove from scaled numerical matrix as well, mind different col names for cat. variables!\n",
    "rmvars = [\"x\",\"y\",\"plot_id\",\"geology_norge1_1\",\"geology_norge1_2\",\"geology_norge1_3\"]\n",
    "idx_rm = [X_dum.columns.get_loc(c) for c in rmvars if c in X_dum]\n",
    "X_nn = np.delete(X_dum_scaled, idx_rm, axis=1)\n",
    "\n",
    "\n",
    "print(X_rf.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST COLLINEARITY BEFORE SUBSETTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(24, 20))\n",
    "corr = X_rf.corr(method = 'spearman')\n",
    "sns.set(font_scale=4)\n",
    "cmap = sns.diverging_palette(h_neg=210, h_pos=350, s=90, l=30, as_cmap=True)\n",
    "\n",
    "b = sns.heatmap(corr, annot = False, cmap=cmap)\n",
    "\n",
    "#b.axes.set_title(\"Title\",fontsize=50)\n",
    "#b.set_xlabel(\"X Label\",fontsize=30)\n",
    "#b.set_ylabel(\"Y Label\",fontsize=20)\n",
    "\n",
    "#b.tick_params([])#labelsize=12)\n",
    "fig.tight_layout()\n",
    "plt.savefig('./Results/FigureFiles/FeatureSpearmanCorr.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test collinearity\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 10))\n",
    "corr = spearmanr(X_rf).correlation\n",
    "corr_linkage = hierarchy.ward(corr)\n",
    "dendro = hierarchy.dendrogram(\n",
    "    corr_linkage, labels=X_rf.columns.tolist(), ax=ax1, leaf_rotation=90\n",
    ")\n",
    "dendro_idx = np.arange(0, len(dendro['ivl']))\n",
    "\n",
    "ax2.imshow(corr[dendro['leaves'], :][:, dendro['leaves']])\n",
    "ax2.set_xticks(dendro_idx)\n",
    "ax2.set_yticks(dendro_idx)\n",
    "ax2.set_xticklabels(dendro['ivl'], rotation='vertical')\n",
    "ax2.set_yticklabels(dendro['ivl'])\n",
    "fig.tight_layout()\n",
    "#plt.savefig('./Results/FigureFiles/CollinearityInputFeats.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use only 1 variable per cluster to reduce importance bias\n",
    "\n",
    "cluster_ids = hierarchy.fcluster(corr_linkage, 1, criterion='distance')\n",
    "cluster_id_to_feature_ids = defaultdict(list)\n",
    "for idx, cluster_id in enumerate(cluster_ids):\n",
    "    cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "'''\n",
    "cluster_id_to_feature_ids contains clusters of correlated variables as seperate lists.\n",
    "Size of lists is determined by \"k\", the second input of hierarchy.fcluster.\n",
    "Higher k --> more correlation required to end up in same list\n",
    "'''\n",
    "selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "\n",
    "X_sel = X_rf.iloc[:, selected_features]\n",
    "print(X_sel.columns)\n",
    "print(X_sel.shape)\n",
    "#clf_sel = RandomForestClassifier(n_estimators=500, random_state=42,n_jobs=-1)\n",
    "#clf_sel.fit(X_train_sel, y_train)\n",
    "#print(\"Accuracy on test data with features removed: {:.2f}\".format(\n",
    "      #clf_sel.score(X_test_sel, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_rf.shape)\n",
    "print(X_rf.columns)\n",
    "print(X_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjust hot encoded data frame\n",
    "\n",
    "# Create a second data frame with hot-one encoded categorical variables\n",
    "X_onehot = X_sel.select_dtypes(exclude=['category'])\n",
    "\n",
    "X_categories = X_sel.select_dtypes(include=[\"category\"])\n",
    "categories = X_categories.columns\n",
    "\n",
    "for cat in categories:\n",
    "    cur = pd.get_dummies(X_categories[cat],prefix=cat)\n",
    "    X_onehot = pd.concat(objs=[X_onehot, cur],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_onehot.info()\n",
    "cont_idx = [i for i,j in enumerate(X_onehot.columns) if X_onehot[j].dtype!='uint8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalize the NN feature matrix\n",
    "X_nn = np.array(X_onehot)\n",
    "X_nn[:,cont_idx] = sc.fit_transform(X_nn[:,cont_idx]) # Omit x/y/plotid and cat. dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SUBSET DATA\n",
    "test_prop = 0.2\n",
    "\n",
    "# Get random indices to subset data\n",
    "train_idx = np.random.choice(range(X_nn.shape[0]), int(X_nn.shape[0]*(1-test_prop)), replace=False)\n",
    "train_idx = sorted(train_idx)\n",
    "test_idx = [i for i in range(X_nn.shape[0]) if i not in train_idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Categorical\n",
    "X_rf_train = X_sel.iloc[train_idx,:]\n",
    "y_rf_train = y_cat.iloc[train_idx]\n",
    "X_rf_test = X_sel.iloc[test_idx,:]\n",
    "y_rf_test = y_cat.iloc[test_idx]\n",
    "\n",
    "### One-hot\n",
    "X_nn_train = X_nn[train_idx,:]\n",
    "y_nn_train = y_dum[train_idx,:]\n",
    "X_nn_test = X_nn[test_idx,:]\n",
    "y_nn_test = y_dum[test_idx,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to create the dense MLP\n",
    "nn = KerasClassifier(build_fn=NeuralNet.initializeNN, verbose=1)\n",
    "\n",
    "### Scikit Learn\n",
    "rf = RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=18, max_features='auto',\\\n",
    "                                    bootstrap=True, oob_score=True, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nn_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nn_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit NN\n",
    "history = nn.fit(X_nn_train, y_nn_train, validation_data=(X_nn_test, y_nn_test), epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit RF\n",
    "rf.fit(X_rf_train, y_rf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_nn = nn.predict(X_nn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_nn = permutation_importance(nn, X_nn_test, y_nn_test, n_repeats=5, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_nn_df = pd.DataFrame(columns=['Mean','Std'])\n",
    "for i in imp_nn.importances_mean.argsort()[::-1]:\n",
    "    if imp_nn.importances_mean[i] - 2 * imp_nn.importances_std[i] > 0:\n",
    "        print(f\"{X_onehot.columns[i]:<40}\" f\"{imp_nn.importances_mean[i]:.3f}\" f\" +/- {imp_nn.importances_std[i]:.3f}\")\n",
    "        imp_nn_df.loc[X_onehot.columns[i],'Mean'] = imp_nn.importances_mean[i]\n",
    "        imp_nn_df.loc[X_onehot.columns[i],'Std'] = imp_nn.importances_std[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_nn_df.to_csv(\"./Results/Tables/NN_permutation.csv\", sep=',',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_nn_df.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_rf = permutation_importance(rf, X_rf_test, y_rf_test, n_repeats=5, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_rf_df = pd.DataFrame(columns=['Mean','Std'])\n",
    "for i in imp_rf.importances_mean.argsort()[::-1]:\n",
    "    if imp_rf.importances_mean[i] - 2 * imp_rf.importances_std[i] > 0:\n",
    "        print(f\"{X_rf_train.columns[i]:<40}\" f\"{imp_rf.importances_mean[i]:.3f}\" f\" +/- {imp_rf.importances_std[i]:.3f}\")\n",
    "        imp_rf_df.loc[X_rf_train.columns[i],'Mean'] = imp_rf.importances_mean[i]\n",
    "        imp_rf_df.loc[X_rf_train.columns[i],'Std'] = imp_rf.importances_std[i]\n",
    "\n",
    "\n",
    "#for i in imp_rf.importances_mean.argsort()[::-1]:\n",
    "#    if imp_rf.importances_mean[i] - 2 * imp_rf.importances_std[i] > 0:\n",
    "#        print(f\"{X_rf_train.columns[i]:<40}\" f\"{imp_rf.importances_mean[i]:.3f}\" f\" +/- {imp_rf.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_rf_df = imp_rf_df.round(3)#.to_csv(\"./Results/Tables/RF_permutation.csv\", sep=',',index=True)\n",
    "imp_rf_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_rf.importances_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred_nn = nn.predict(X_nn_test)\n",
    "y_pred_rf = rf.predict(X_rf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_nn = nn.score(X_nn_test, y_nn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_rf = accuracy_score(y_pred_rf, y_rf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_nn)\n",
    "print(acc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imp_rf_df.index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(NeuralNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put everything into one data frame and print as latex\n",
    "permutation_df = pd.DataFrame(columns=['RandomForestClassifier','MultiLayerPerceptron'])\n",
    "permutation_df.loc['TestAccuracy'] = [acc_rf, acc_nn]\n",
    "\n",
    "permutation_vars = pd.DataFrame(columns=['RFvarName','RFval','NNvarName','NNval'])\n",
    "for i in range(5):\n",
    "    cur_var_rf = imp_rf_df.index[i]\n",
    "    cur_var_nn = imp_nn_df.index[i]\n",
    "    permutation_vars.loc[str(i+1)] = [cur_var_rf,\n",
    "                                      str(np.round(imp_rf_df.loc[cur_var_rf,'Mean'],3))+\" (±\"+str(np.round(imp_rf_df.loc[cur_var_rf,'Std'],3))+\")\",\n",
    "                                      cur_var_nn,\n",
    "                                      str(np.round(imp_nn_df.loc[cur_var_nn,'Mean'],3))+\" (±\"+str(np.round(imp_nn_df.loc[cur_var_nn,'Std'],3))+\")\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#permutation_vars.to_csv('./Results/Tables/VarImpComparison.csv', sep=',',index=True)\n",
    "#permutation_df.to_csv('./Results/Tables/VarImpAcc.csv', sep=',',index=True)\n",
    "\n",
    "print(permutation_vars.to_latex(caption=\"The five most important variables according to permutation importance with n=5 random permutations.\"))\n",
    "print(permutation_df.to_latex(caption=\"The five most important variables according to permutation importance with n=5 random permutations.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
